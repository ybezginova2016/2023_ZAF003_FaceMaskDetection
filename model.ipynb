{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a6d31fb9",
      "metadata": {
        "id": "a6d31fb9"
      },
      "source": [
        "# ZAF003_CV_FaceMaskDetection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24278daf",
      "metadata": {
        "id": "24278daf"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Face mask detection is an image processing and computer vision task that involves detecting whether a person in an image or video is wearing a face mask or not. With the ongoing COVID-19 pandemic, face masks have become a crucial tool in preventing the spread of the virus. Face mask detection systems can be used in public spaces such as airports, malls, and hospitals to ensure that individuals are following safety protocols."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f0d519",
      "metadata": {
        "id": "e9f0d519"
      },
      "source": [
        "## Methodology\n",
        "\n",
        "- Import Libraries: First, you need to import the necessary libraries, including PyTorch, NumPy, OpenCV, and Matplotlib.\n",
        "\n",
        "- Load the Data: Load the dataset of images with labeled classes (mask or no mask) and split it into training, validation, and testing sets. You can use PyTorch's DataLoader to load and preprocess the data.\n",
        "\n",
        "- Load Pretrained Model: Load the pre-trained ResNet model from PyTorch's model zoo. ResNet is a popular choice for image classification tasks due to its superior performance on large datasets.\n",
        "\n",
        "- Freeze Layers: Freeze the weights of the convolutional layers in the pre-trained model to prevent them from being updated during training.\n",
        "\n",
        "- Replace the Classifier: Replace the final fully connected layer of the pre-trained model with a new classifier that has the appropriate number of output classes (2 for mask and no mask).\n",
        "\n",
        "- Define Loss Function: Define the loss function for the binary classification problem, such as binary cross-entropy loss.\n",
        "\n",
        "- Define Optimizer: Define the optimizer, such as stochastic gradient descent (SGD), to update the weights of the new classifier.\n",
        "\n",
        "- Train the Model: Train the new classifier using the training set and validate it on the validation set. Use PyTorch's training loop to perform iterations of forward and backward propagation, and update the model's weights based on the loss and gradients.\n",
        "\n",
        "- Test the Model: Test the final trained model on the testing set to evaluate its performance.\n",
        "\n",
        "- Fine-Tune the Model: Fine-tune the model by unfreezing some of the earlier layers in the pre-trained model and continuing to train the model with a lower learning rate.\n",
        "\n",
        "- Deploy the Model: Deploy the trained model for inference on new images or videos. Use the trained model to detect face masks in real-time scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06e1898",
      "metadata": {
        "id": "b06e1898"
      },
      "source": [
        "## Business Segments\n",
        "1. Healthcare\n",
        "2. Safety"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42eb7f90",
      "metadata": {
        "id": "42eb7f90"
      },
      "source": [
        "## Data\n",
        "Face Mask Detection Datset - [Link](https://www.kaggle.com/datasets/omkargurav/face-mask-dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88b0044",
      "metadata": {
        "id": "a88b0044"
      },
      "source": [
        "## Papers\n",
        "1. Resnet Architecture - [Link](https://arxiv.org/abs/1512.03385)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4c7e9c",
      "metadata": {
        "id": "ca4c7e9c"
      },
      "source": [
        "## Demo Link"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c38ba2a",
      "metadata": {
        "id": "4c38ba2a"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4a889ddf",
      "metadata": {
        "id": "4a889ddf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYM0fLVhrty-",
        "outputId": "42c6536c-27b5-4b02-dcb1-263589712a49"
      },
      "id": "pYM0fLVhrty-",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Global Variables"
      ],
      "metadata": {
        "id": "WceRObcUf2LN"
      },
      "id": "WceRObcUf2LN"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0e42960d",
      "metadata": {
        "id": "0e42960d"
      },
      "outputs": [],
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Image Data\"\n",
        "\n",
        "# Defining model name\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Helper Functions"
      ],
      "metadata": {
        "id": "Q3ZEv-aJf-_-"
      },
      "id": "Q3ZEv-aJf-_-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to save best model\n",
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's \n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
        "            torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Image Data/best_model.pth')"
      ],
      "metadata": {
        "id": "_DgXiQ1LbjBM"
      },
      "id": "_DgXiQ1LbjBM",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_model = SaveBestModel()"
      ],
      "metadata": {
        "id": "8AgBWNfvbn1L"
      },
      "id": "8AgBWNfvbn1L",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                save_best_model(\n",
        "                epoch_loss, epoch, model, optimizer, criterion\n",
        "            )\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "HFi1NaE6s-Zh"
      },
      "id": "HFi1NaE6s-Zh",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "q62sxfxrtbUx"
      },
      "id": "q62sxfxrtbUx",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pre-trained model"
      ],
      "metadata": {
        "id": "Ibfa-z-7gFKm"
      },
      "id": "Ibfa-z-7gFKm"
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet34\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet34(weights=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "109ae956089740cfa46b9d4697866cc1",
            "6b2e190895bb4257b7dc7913d27e3a8a",
            "68825160e133448d87483aafdc81b9d1",
            "39442b90741c4345b182df6b23614012",
            "8cd6af49173e4908bf6f0a43e4654416",
            "973fb20081aa4cea86d4f8049a110b31",
            "9d1601dd2ab748abbc8236b08c3095d7",
            "65f680c448d54c308ff383ccfe2c56d6",
            "e301a09a10244dda9ed00e0ad82ff1cb",
            "3498a24834e244aa874a0ab97abfb5a4",
            "39f588cb96c64d77a581822e4d66c2c2"
          ]
        },
        "id": "wgIE0qu-TXjk",
        "outputId": "3d49dd70-401d-4b65-9a82-e99f8a08af79"
      },
      "id": "wgIE0qu-TXjk",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "109ae956089740cfa46b9d4697866cc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid model name, exiting...\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "gCOcypNsgLvz"
      },
      "id": "gCOcypNsgLvz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'with mask': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'without mask': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create datset\n",
        "dataset = ImageFolder(data_dir, data_transforms['with mask'])\n",
        "\n",
        "# Creating Train and validation dataset\n",
        "val_pct = 0.1\n",
        "val_size = int(val_pct * len(dataset))\n",
        "train_ds, valid_ds = random_split(dataset, [len(dataset) - val_size, val_size])\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2)\n",
        "valid_dl = DataLoader(valid_ds, batch_size, num_workers=2, shuffle=True)\n",
        "\n",
        "dataloaders_dict = {'train': train_dl,\n",
        "                    'val': valid_dl}\n",
        "\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krWZrLGNthJX",
        "outputId": "8776f1a3-df9a-4bf6-efb9-dc85f2eb3d15"
      },
      "id": "krWZrLGNthJX",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "aKRknLJEgUfj"
      },
      "id": "aKRknLJEgUfj"
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwgRhIfkthNg",
        "outputId": "983015e2-537a-48da-bbf0-8830586eba45"
      },
      "id": "hwgRhIfkthNg",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, \n",
        "                             optimizer_ft, num_epochs=num_epochs, \n",
        "                             is_inception=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-tGQlZlthRZ",
        "outputId": "f202ab3b-397e-42af-d6c3-01ee32986f52"
      },
      "id": "z-tGQlZlthRZ",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 0.4700 Acc: 0.7950\n",
            "val Loss: 0.3238 Acc: 0.8905\n",
            "\n",
            "Best validation loss: 0.3238043616505435\n",
            "\n",
            "Saving best model for epoch: 1\n",
            "\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.3354 Acc: 0.8838\n",
            "val Loss: 0.2516 Acc: 0.9197\n",
            "\n",
            "Best validation loss: 0.25164739711441264\n",
            "\n",
            "Saving best model for epoch: 2\n",
            "\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.2978 Acc: 0.8886\n",
            "val Loss: 0.1964 Acc: 0.9416\n",
            "\n",
            "Best validation loss: 0.19638906343139872\n",
            "\n",
            "Saving best model for epoch: 3\n",
            "\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.2695 Acc: 0.8862\n",
            "val Loss: 0.1356 Acc: 0.9635\n",
            "\n",
            "Best validation loss: 0.13564982591536792\n",
            "\n",
            "Saving best model for epoch: 4\n",
            "\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.2397 Acc: 0.9104\n",
            "val Loss: 0.1519 Acc: 0.9562\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.2542 Acc: 0.8959\n",
            "val Loss: 0.1696 Acc: 0.9270\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.2423 Acc: 0.9031\n",
            "val Loss: 0.1665 Acc: 0.9416\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.2181 Acc: 0.9177\n",
            "val Loss: 0.1809 Acc: 0.9489\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.2153 Acc: 0.9177\n",
            "val Loss: 0.1461 Acc: 0.9489\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.2123 Acc: 0.9249\n",
            "val Loss: 0.1482 Acc: 0.9343\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.2100 Acc: 0.9056\n",
            "val Loss: 0.1554 Acc: 0.9197\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.2195 Acc: 0.9144\n",
            "val Loss: 0.1449 Acc: 0.9562\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.2063 Acc: 0.9128\n",
            "val Loss: 0.1379 Acc: 0.9562\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.2254 Acc: 0.9048\n",
            "val Loss: 0.1094 Acc: 0.9708\n",
            "\n",
            "Best validation loss: 0.10941007298274633\n",
            "\n",
            "Saving best model for epoch: 14\n",
            "\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.2031 Acc: 0.9120\n",
            "val Loss: 0.1178 Acc: 0.9562\n",
            "\n",
            "Training complete in 2m 39s\n",
            "Best val Acc: 0.970803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading model"
      ],
      "metadata": {
        "id": "USy6Sxd9iS57"
      },
      "id": "USy6Sxd9iS57"
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Image Data/best_model.pth\")"
      ],
      "metadata": {
        "id": "JRcTmqz_gX95"
      },
      "id": "JRcTmqz_gX95",
      "execution_count": 42,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "109ae956089740cfa46b9d4697866cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b2e190895bb4257b7dc7913d27e3a8a",
              "IPY_MODEL_68825160e133448d87483aafdc81b9d1",
              "IPY_MODEL_39442b90741c4345b182df6b23614012"
            ],
            "layout": "IPY_MODEL_8cd6af49173e4908bf6f0a43e4654416"
          }
        },
        "6b2e190895bb4257b7dc7913d27e3a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973fb20081aa4cea86d4f8049a110b31",
            "placeholder": "​",
            "style": "IPY_MODEL_9d1601dd2ab748abbc8236b08c3095d7",
            "value": "100%"
          }
        },
        "68825160e133448d87483aafdc81b9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f680c448d54c308ff383ccfe2c56d6",
            "max": 87319819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e301a09a10244dda9ed00e0ad82ff1cb",
            "value": 87319819
          }
        },
        "39442b90741c4345b182df6b23614012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3498a24834e244aa874a0ab97abfb5a4",
            "placeholder": "​",
            "style": "IPY_MODEL_39f588cb96c64d77a581822e4d66c2c2",
            "value": " 83.3M/83.3M [00:01&lt;00:00, 63.1MB/s]"
          }
        },
        "8cd6af49173e4908bf6f0a43e4654416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "973fb20081aa4cea86d4f8049a110b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1601dd2ab748abbc8236b08c3095d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f680c448d54c308ff383ccfe2c56d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e301a09a10244dda9ed00e0ad82ff1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3498a24834e244aa874a0ab97abfb5a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f588cb96c64d77a581822e4d66c2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}